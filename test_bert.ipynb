{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8648d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2411eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\", encoding='ISO-8859-1')\n",
    "df_train.dropna(subset=['text', 'sentiment'], inplace=True)\n",
    "df_test = pd.read_csv(\"data/test.csv\", encoding='ISO-8859-1')\n",
    "df_test.dropna(subset=['text', 'sentiment'], inplace=True)\n",
    "\n",
    "# Ensure text data is a list of strings and drop missing values\n",
    "df_train['text'] = df_train['text'].astype(str)\n",
    "df_test['text'] = df_test['text'].astype(str)\n",
    "\n",
    "df_train = df_train.rename(columns={'sentiment': 'label'})\n",
    "df_test = df_test.rename(columns={'sentiment': 'label'})\n",
    "\n",
    "df_train['label'] = df_train['label'].apply(lambda x: 2 if x == 'positive' else 1 if x == 'neutral' else 0)\n",
    "df_test['label'] = df_test['label'].apply(lambda x: 2 if x == 'positive' else 1 if x == 'neutral' else 0)\n",
    "\n",
    "\n",
    "df_train['label'] = df_train['label'].astype(int)\n",
    "df_test['label'] = df_test['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edf9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google-bert/bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6061d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 27480/27480 [00:01<00:00, 17325.96 examples/s]\n",
      "Map: 100%|██████████| 3534/3534 [00:00<00:00, 34046.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(df_train[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(df_test[['text', 'label']])\n",
    "\n",
    "# Adding short max length to lower training time\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d01cf5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "218554ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model = model.to(device)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],  # BERT uses query, key, value in attention\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\" + model_name,\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #gradient_checkpointing=True,\n",
    "    num_train_epochs=30,\n",
    "    dataloader_num_workers=5,\n",
    "    #logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c04d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a compute metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(p.label_ids, preds)\n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32a4c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossAccuracyLogger(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.eval_accuracy = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                self.train_loss.append((state.epoch, logs[\"loss\"]))\n",
    "            if \"eval_accuracy\" in logs:\n",
    "                self.eval_accuracy.append((state.epoch, logs[\"eval_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b3af93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "logger_callback = LossAccuracyLogger()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[logger_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "788992c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料集中最長的 Token 長度是: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25991 (\\N{CJK UNIFIED IDEOGRAPH-6587}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26412 (\\N{CJK UNIFIED IDEOGRAPH-672C}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25976 (\\N{CJK UNIFIED IDEOGRAPH-6578}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 38263 (\\N{CJK UNIFIED IDEOGRAPH-9577}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24230 (\\N{CJK UNIFIED IDEOGRAPH-5EA6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20296 (\\N{CJK UNIFIED IDEOGRAPH-4F48}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/Users/kevinhsu/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22294 (\\N{CJK UNIFIED IDEOGRAPH-5716}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料集中 95% 的文本 Token 長度小於或等於: 38\n"
     ]
    }
   ],
   "source": [
    "# --- 在 Cell 2 和 Cell 3 執行後，加入這個新的 Cell ---\n",
    "\n",
    "# 計算訓練集中每個文本的 Token 長度\n",
    "train_token_lengths = [len(tokenizer.encode(text)) for text in df_train['text']]\n",
    "# 計算測試集中每個文本的 Token 長度\n",
    "test_token_lengths = [len(tokenizer.encode(text)) for text in df_test['text']]\n",
    "\n",
    "# 合併所有長度\n",
    "all_token_lengths = train_token_lengths + test_token_lengths\n",
    "\n",
    "# 找出最長的 Token 長度\n",
    "max_len = max(all_token_lengths)\n",
    "\n",
    "print(f\"資料集中最長的 Token 長度是: {max_len}\")\n",
    "\n",
    "# (選用) 繪製長度分佈圖，幫助決定 max_length\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_token_lengths, bins=50) # bins 可以調整，觀察更細緻的分佈\n",
    "plt.title('Token 長度分佈圖')\n",
    "plt.xlabel('Token 長度')\n",
    "plt.ylabel('文本數量')\n",
    "# 可以在圖上標示一個常用的百分位數，例如 95% 或 99%\n",
    "percentile_95 = np.percentile(all_token_lengths, 95)\n",
    "plt.axvline(percentile_95, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.text(percentile_95 * 1.05, plt.ylim()[1] * 0.9, f'95th percentile: {int(percentile_95)}')\n",
    "plt.show()\n",
    "\n",
    "print(f\"資料集中 95% 的文本 Token 長度小於或等於: {int(percentile_95)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08c68391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='6450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  10/6450 00:32 < 7:16:22, 0.25 it/s, Epoch 0.04/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.892400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.560900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/peft/peft_model.py:1559\u001b[39m, in \u001b[36mPeftModelForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m peft_config.peft_type == PeftType.POLY:\n\u001b[32m   1558\u001b[39m             kwargs[\u001b[33m\"\u001b[39m\u001b[33mtask_ids\u001b[39m\u001b[33m\"\u001b[39m] = task_ids\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1565\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1566\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1570\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1572\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1675\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1667\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1668\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1669\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1670\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1671\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1672\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1673\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1675\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1685\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1687\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1689\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    639\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:554\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    552\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m    553\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/AIT 526/Skynet/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1735\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1732\u001b[39m             tracing_state.pop_scope()\n\u001b[32m   1733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m1735\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1736\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1737\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61fb12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16584683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4960556924343109, 'eval_accuracy': 0.7954159592529711, 'eval_runtime': 20.3385, 'eval_samples_per_second': 173.759, 'eval_steps_per_second': 5.458, 'epoch': 2.9848661233993017}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Unpack the epoch and values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_epochs, train_losses = \u001b[38;5;28mzip\u001b[39m(*logger_callback.train_loss)\n\u001b[32m      9\u001b[39m eval_epochs, eval_accuracies = \u001b[38;5;28mzip\u001b[39m(*logger_callback.eval_accuracy)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unpack the epoch and values\n",
    "train_epochs, train_losses = zip(*logger_callback.train_loss)\n",
    "eval_epochs, eval_accuracies = zip(*logger_callback.eval_accuracy)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_epochs, train_losses, marker='o')\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Plot eval accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_epochs, eval_accuracies, marker='o', color='green')\n",
    "plt.title(\"Evaluation Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skynet",
   "language": "python",
   "name": "poetry_env_skynet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
